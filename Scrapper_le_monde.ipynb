{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PghiYhVItcZd"
   },
   "outputs": [],
   "source": [
    "from urllib.error import HTTPError\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BybNKXyJs8WG"
   },
   "source": [
    "# Create archive links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WF79zdzrs6Mx"
   },
   "outputs": [],
   "source": [
    "def create_archive_links(year_start, year_end, month_start, month_end, day_start, day_end):\n",
    "    archive_links = {}\n",
    "    for y in range(year_start, year_end + 1):\n",
    "        dates = [str(d).zfill(2) + \"-\" + str(m).zfill(2) + \"-\" +\n",
    "                 str(y) for m in range(month_start, month_end + 1) for d in\n",
    "                 range(day_start, day_end + 1)]\n",
    "        archive_links[y] = [\n",
    "            \"https://www.lemonde.fr/archives-du-monde/\" + date + \"/\" for date in dates]\n",
    "    return archive_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OsIuDObHtBPj"
   },
   "outputs": [],
   "source": [
    "#create_archive_links(2006,2020,1, 12, 1, 31)\n",
    "archive_links = create_archive_links(2006,2020,1, 12, 1, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njRlf3NUtenK"
   },
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VfNFxGecth7m"
   },
   "outputs": [],
   "source": [
    "def get_articles_links(archive_links):\n",
    "  '''Each article is in a <section> having a class named teaser and here \n",
    "  I also filter all the non free articles having a span with class icon__premium. \n",
    "  All the links containing the word en-direct are also filtered because they are videos. '''\n",
    "  links_non_abonne = []\n",
    "  for link in archive_links:\n",
    "      try:\n",
    "          html = urlopen(link)\n",
    "      except HTTPError as e:\n",
    "          print(\"url not valid\", link)\n",
    "      else:\n",
    "          soup = BeautifulSoup(html, \"html.parser\")\n",
    "          news = soup.find_all(class_=\"teaser\")\n",
    "          # condition here : if no span icon__premium (abonnes)\n",
    "          for item in news:\n",
    "              if not item.find('span', {'class': 'icon__premium'}):\n",
    "                  l_article = item.find('a')['href']\n",
    "                  # en-direct = video\n",
    "                  if 'en-direct' not in l_article:\n",
    "                      links_non_abonne.append(l_article)\n",
    "  return links_non_abonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "d-xe-0AQvWwm"
   },
   "outputs": [],
   "source": [
    "def get_single_page(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        print(\"url not valid\", url)\n",
    "    else:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        try:\n",
    "          text_title = soup.find('h1')\n",
    "        except:\n",
    "          text_title = None\n",
    "        try:\n",
    "          text_body = soup.article.find_all([\"p\", \"h2\"], recursive=False)\n",
    "        except:\n",
    "          text_body = None\n",
    "        return (text_title, text_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rLEMt9Bq0aFc"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Year', 'Html'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "JKMp-7Iqt2iM",
    "outputId": "801774bd-76ca-4533-87f1-d3ef9b620499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  2006\n",
      "processing:  2007\n",
      "processing:  2008\n",
      "processing:  2009\n",
      "processing:  2010\n",
      "processing:  2011\n",
      "processing:  2012\n",
      "processing:  2013\n",
      "processing:  2014\n",
      "processing:  2015\n",
      "processing:  2016\n",
      "processing:  2017\n",
      "processing:  2018\n",
      "processing:  2019\n",
      "processing:  2020\n"
     ]
    }
   ],
   "source": [
    "for year,links in archive_links.items():\n",
    "    print(\"processing: \",year)\n",
    "    article_links_list = get_articles_links(links)\n",
    "    temp = pd.DataFrame({'Year': [year]*len(article_links_list), 'Html': article_links_list})\n",
    "    df = df.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Y1z1A6sR5Y-P"
   },
   "outputs": [],
   "source": [
    "def titre_Body(x):\n",
    "    try:\n",
    "        get = get_single_page(x)\n",
    "        try:\n",
    "            titre = get[0].get_text\n",
    "        except:\n",
    "            \n",
    "            titre = None\n",
    "        try:\n",
    "            body = \" \".join([i.get_text() for i in get[1]])\n",
    "        except:\n",
    "            body = None\n",
    "    except:\n",
    "        titre = None\n",
    "        body = None\n",
    "    return titre, body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vz875K1B5PfZ",
    "outputId": "df2dac14-1251-44a8-a925-b29de7d3f5ba"
   },
   "outputs": [],
   "source": [
    "df['Titre'], df['Body'] = zip(*df['Html'].map(titre_Body))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7usIbbgP_tpX"
   },
   "outputs": [],
   "source": [
    "def keep(x,nbr):\n",
    "  if len(x)>nbr:\n",
    "    retour=1\n",
    "  else:\n",
    "    retour=0\n",
    "  return retour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7Qj568_ApIv"
   },
   "outputs": [],
   "source": [
    "df['Titre_OK'] = df['Titre'].apply(lambda x: keep(x,10))\n",
    "df['Body_OK'] = df['Body'].apply(lambda x: keep(x,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5XZuVBXDEMt",
    "outputId": "dc0395ae-b69d-41b7-c04d-5b455727ce29"
   },
   "outputs": [],
   "source": [
    "df['Body_OK'].sum()/len(df['Body'])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.to_csv('out.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Scrapper_le_monde.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
